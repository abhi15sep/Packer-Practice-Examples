
The HashiCorp company provides a suite of (mostly) free and open source virtual server provisioning tools. One of the strengths of these tools is their ability to provision a single image (or "artifact" in Packer terms) and deploy it over and over again as containers (or "boxes" in Vagrant terms) over multiple providers. This way, you can precisely define an operating system environment and have it reliably run for different purposes. 
You might, for instance, want to have the development team run it locally on VirtualBox in complete isolation, let a remote team test it over and over again within their own LXC environment, and then move it to production on a cloud provider like AWS. Not only can you be sure that all your teams are seeing the exact same image, but any changes you commit to one iteration, are automatically picked up in the others. 

You can have Vagrant produce boxes based on a number of sources, but HashiCorp recommends building them in Packer. You can install Packer by downloading and then unzipping the latest archive into a new directory - I'm using /usr/local/packer. 
This will give you a packer binary and three kinds of scripts that can be called from your Packer template: the first script type is called a builder, which is used to generate machine images meant to run on a specific platform like VirtualBox, AWS, or Docker. Provisioners configure software and system settings for a running image before it's generated. And post processors - the third type - generate artifacts out of the builders' images.
All those components are brought together in a template.json file. Here's a very simple example borrowed from Packer's documentation pages: 

The template defines amazon-ebs as its builder and passes authentication and configuration information - including an AWS region, Amazon Machine Image (AMI), and instance type. You could either include your AWS keys in the template itself or pass them as variables from the command line. 
You get packer to build your artifacts using packer build: 
The output of this command will be a usable image - an AWS AMI, in this case. Naturally, if your template included an image you'd like to manage in Vagrant, you could push it to Atlas so Vagrant can get to work on it: 

Before you can install Vagrant, you have to decide which provider you want to use and then make sure it's properly installed on your host. As I already mentioned, most people seem to use Oracle's VirtualBox as a provider, but since there are plenty of online resources to help you through that process, I thought we'd do something just a bit different and quickly set up Vagrant to use an LXC container running Ubuntu 14.04. You can also use VMware, Docker, and Hyper-V as providers
We'll start by installing LXC itself (along with redir - for those planning to use port forwarding). 
It's a good idea to avoid the Vagrant package that's in the Debian repositories - especially when you're using older (LTS) distributions - so I'll directly download and install the latest Vagrant release from their web site: 
With Vagrant installed, I'll have it install our vagrant-lxc plugin for us: 
Vagrant init will generate a Vagrantfile config file in the current directory. We'll point our init operation to a handy online project that already exists. 
It's probably a good idea to take a look through the Vagrantfile to get an idea of how it works and how this particular project is built. One parameter that should interest you immediately is config.vm.box. By default, config.vm.box will be given the value "base", but our init operation wrote config as "fgrehm/precise64-lxc". 
Next, because there could be complications during the launch process involving root authentications, you might want to run vagrant lxc sudoers, which will create a script in your /etc/sudoers.d/ directory effectively whitelisting LXC-related commands. As with all scripts you pick up from the Internet (or from Pluralsight courses, for that matter) you really must read them through carefully to make sure that they're not doing anything dangerous. It's your system so, ultimately, it's your responsibility. 
Finally, we're ready to launch our Vagrant project (or "box", as Vagrants call them). In this case, we'll point "up" to our LXC provider. 
Now that you've got a box running, we can work with it from the command line using Vagrant. Run vagrant on its own for a quick list of commands. Among them, you will find Vagrant ssh, which will open a secure shell so you can administrate the running container.
But that kind of defeats the purpose of Vagrant, which is really all about scripting things so you don't have to manage them directly. 
Vagrant lets you push boxes that you've configured to Hashicorp's Atlas so others can access it. You would set the Vagrantfile config.push.define attribute with your Atlas user name and application name so Vagrant will know what to do with it. 
Vagrant status will return the status of the current running box, and let's you share your environment with others. As you can see, vagrant halt, suspend, and up will perform just the kind of tasks you'd expect arguments named that way to perform.

Let's review what we know about Vagrant. A virtual image, in Packer terms, is known as an artifact. An artifact can be deployed in virtual containers called boxes. Ideally, you use HashiCorp's Packer to build your boxes.
Packer provides three kinds of scripts: builders - to generate images, provisioners, to generate configuration profiles, and post processors to actually create artifacts. The whole process is orchestrated by a template.json file that's created by "packer build" and launched by "packer push".
Once you've got a finished artifact that's been pushed to an archive, you can call it using "vagrant init" to generate a Vagrantfile. Based on the Vagrantfile, the container is then launched using vagrant up. "vagrant ssh" will open an ssh session within your new, running container.

As a way of placing everything we've learned about managing virtual servers into a real-world context, I'm going to present a kind of schematic diagram to illustrate how all the bits and pieces we could use for our college's IT virtualized deployment might work together. Naturally, this is only one of many possible combinations of tools and processes we could use, but I think it should be helpful. 
We would start by building two physical servers running, say, the Xen hypervisor, each with two separate domains (or guests): one domain will contain the software stack necessary to run a webserver with the Moodle Learning Management System to provide students with all their online course resources. The second domain will provide a separate webserver for registration and student account management. Both domains will connect to separate databases on a single database server (that's regularly backed up and replicated). The reason we'll use two separate but identical physical servers is to cover us for the certainty that at least one of them will one day fail.
We will also place an inexpensive server running the Shorewall firewall package at the perimeter of our network to securely control all inbound and outbound traffic.
The college has two subdomains to its www.myschool.edu address: content.myschool.edu for the Moodle server, and admin.myschool.edu for registration services. Using our DNS server, we'll point our domains to an HAProxy load balancer which will redirect traffic to appropriate destinations.
As a new academic semester approaches, we anticipate increased demand for our admin services as new students register and everyone signs up for courses. To meet the demand, we'll clone our registration front end webserver domain on each physical server and start up, say, two new copies of it on the same network. Each domain will already have the connectivity and authentication information it will need to access the backend database.
To make sure that all six registration servers (three running on each physical server) are utilized, we'll tell our HAProxy load balancer about our new domains it can intelligently distribute incoming traffic - using an algorithm we create - to send each request to the domain that currently has the highest free capacity to handle it. Once demand drops back down, we will simply kill off the four cloned domains.
We would, of course, do the exact same thing, but in reverse, to handle demand fluctuation against our Moodle server.
For a more resilient and fault tolerant infrastructure, we might want to place separate physical servers off-site - or in the cloud - that will be ready to take over if electrical power or network connectivity to our local data center goes down. Once we've got hypervisors running on our new servers, it will be a simple matter of establishing a secure connection between our sites and then migrating each of our two local domains - admin and Moodle - to the hypervisor running remotely.


