
KVM, by contrast to Xen, works from far deeper "inside" the Linux software stack, taking direct control of Linux tools like the scheduler and memory management. Unlike Xen, KVM only supports processors using the vt/svm instruction set (for HVM) and does not provide paravirtualization options. That means that, by and large, it will run faster on those systems with which it's compatible - but that it's compatible with fewer systems. 
But where you see KVM, you're also likely to see references to a similar resource called QEMU. Where exactly does that fit in? Besides being able to act as a hypervisor on its own, QEMU's real strength is as an emulator. KVM, in its hypervisor virtualization role, can use QEMU for its emulation powers to compliment its own hardware acceleration features. The whole, as they say, can be greater than the sum of its parts. 
Because of the way KVM works with your hardware, you'll need to make sure that the physical machine you're planning to use as a KVM host, supports hardware virtualization. Besides the BIOS setting (which we've already discussed), you can also check this from within a running Linux system, using kvm-ok or egrep.
Any number returned above 0 is good. If you do get a 0, KVM might still run, but it will be a lot slower, because acceleration will not be available.
It's also a good idea to be sure which hardware architecture - 64 or 32-bit - you're working with: 
Even if your hardware profile is up to the task, you'll still need to add the appropriate module to the Linux kernel. If they're not already there, you should add the kvm and either kvm-intel or kvm-amd kernel modules. 
If those modules fail to load (and there's no /dev/kvm device visible in the file system), then there's a good chance your CPU isn't compatible with this kind of virtualization. 
Otherwise, you're ready to install KVM itself: 
Test your installation by trying out virsh: 
 
With some exceptions (which we'll discuss later), you'll find the automatically generated .XML files that define your guests' properties in the /etc/libvirt/qemu/ directory. The guest-specific source files that are similarly generated by virt-install are kept in /var/lib/libvirt/images/. Logs can be saved to either /var/log/libvirt/qemu/ and ~/.virtinst/.  

It's no secret that virtualization platforms have a well-deserved reputation for being complicated. But there are two things that can make getting started with KVM just a bit more challenging than some of the others: 
One, there are quite a few overlapping management tools available with similar - but not identical - functionality. And, two, from time to time, the names used for the key binaries will change depending on which distribution and release you're using. 
I'll do my best to describe all the most important packages currently out there, but you might want to read the documentation that's specific to the Linux distribution and release you're planning to use. As before, of course, there are GUI tools like the Virtual Machine Manager (otherwise known as virt-manager) to help with administration, but we'll focus on the command line.
As we've seen, both Xen and KVM run on top of libvirt's libraries, so libvirt's built-in management tools can directly perform many common KVM-related tasks. Installing the libvirt-bin package (on Debian-based systems, at least) will provide the virsh shell. 
With virsh, for instance, you can list the guests running on your system, and adding --all will include even stopped guests. 
There are always going to be guests who will overstay their welcome, so using virsh destroy against the guest ID (as displayed by virsh list) will quickly put an end to their thoughtlessness. 
virsh help will tell you everything you need to know that we haven't covered either here or back in the libvirt module. 

You can provision and, if necessary, even kickstart new guests using virt-install, which is included with the virt-manager package. 
Here's a sample command line argument combination: [display slide(s) of example][copy ISO to client]

As you carefully read through the arguments, you'll see I've given this guest the name "ubuntu-vm", provisioned it with 1024 MB of RAM and two virtual CPUs, and set the disk path for its image (naming the new image ubuntu-vm.img). I also pointed virt-install to the location of the Ubuntu 14.04 ISO file I previously downloaded. Since I'll be working with a headless server, I set --graphics to "none", and added a console type to --extra-args - because otherwise, I'll have no way to interact with the operating system installation process. the console=tty0 means that I will be able to complete the OS installation directly within a terminal shell session (and without the need to log in remotely using a VNC tool). The value of network bridge should match the network bridge configured on your host.
The default value for --connect= in this example is qemu:///system. qemu:///system will create KVM and QEMU guests, but you could also specify Xen or LXC, among others. 
In fact, virt-install will let you get away with an absolute minimum of three arguments: --name, --ram, and some kind of guest storage (you can choose between --disk, --filesystem, and --nodisks). 

You can safely edit a guest's XML configuration file (even while the VM is running) through virsh edit: 
By default, this will open the file in the vi text editor. 
Virsh can also be used to start a stopped guest. 

For a highly automated guest creation process, vmbuilder - or its Ubuntu-affiliated ubuntu-vm-builder wrapper - definitely has its charms. 
On the surface, the command syntax may look a lot like virt-install, but there are two big differences. Rather than pointing the script to an OS image somewhere on the host, you just specify the distro you want (Ubuntu, in my case) and the release version (Trusty), and vmbuilder will download it for you. But perhaps the bigger difference is the fact that there won't be any .XML file saved to your file system: the --libvirt argument tells libvirt to manage the configuration directly. 

Note: you will not be able to use virsh commands to manage your guests generated using vmbuilder unless you include the --libvirt qemu:///system argument. To start your newly-built guest up, you can use either, or virsh. 
If you're building a more complex guest profile, you might like to load your arguments from a config file rather than including the whole, long string on the command line. This can be done with a vmbuilder.cfg file. By default, vmbuilder will look for either /etc/vmbuilder.cfg or ~/.vmbuilder.cfg, but you can write your own and call it from the command line. 
Here's an example of a simple .cfg file.

As you can tell from the addpkg line, this guest is going to be a LAMP webserver, so we're installing all the packages we'll need at build-time, saving us the effort later.

Building new guests using what we'll call the "KVM" way is a two step process. First, you'll use qemu-img to create a new image or modify or convert an old one. Then you'll use qemu-kvm to set up a virtual machine that will start up the installation. 
Of course, by "qemu-kvm" I mean "qemu-system-x86_64" - which is its new name. In the meantime, some systems offer kvm as a wrapper that executes qemu-system-x86_64 with the -enable-kvm argument. Just don't confuse the kvm wrapper with the old kvm binary with a somewhat different syntax. 
So let's see how these two steps work. You create a disk image with qemu-img, where "my-disk" is the name of the image you'd like to create, the maximum size of our image will be 6 GB, and qcow2 is the file format. qcow, by the way, stands for "QEMU Copy On Write". 
	sudo qemu-img create -f qcow2 /var/lib/libvirt/images/my-disk.img 6G 
We're now ready for step two. Here's how we'll build our VM: 

To explain: using "kvm" (although, of course, the command used on your particular system may differ), we'll call our new guest "my-VM" and assign it a machine type of pc. For a complete list of available machine types, you can run kvm -M ?: 
-m defines the maximum memory you want to make available to the guest. smp 2 provides two processors, the first -drive file= value defines how we will access our virtual hard drive, and the second -drive file= sets up a virtual CDRom drive containing the base ISO image. The -net argument establishes a network connection for your guest - again: make sure that the value for bridge matches your host's actual bridge. Finally, since I'm using a headless server, I'll set -vga to none. 
This will kick off the OS installation process and generate a .XML file in the /etc/libvirt/qemu/ directory. 
Subsequently, whenever we'd like to start our machine back up, we could use the very same command line arguments, but without the second -drive file= line (because we won't need the installation disk any more). 
While working with QEMU, you can open the KVM monitor console and interact with your clients in ways that might be difficult or even impossible using a regular headless server. You launch the monitor from within a client shell session by pressing CTRL+ALT, and then SHIFT+2, and a new console will open on your desktop. SHIFT+1 will close the console. You can also access the console from the command line using something like qemu-system -monitor standard io: 
...this approach allows you to add command line arguments, like that -monitor which specified a console target. Consult man qemu-system-x86_64 for details on the kinds of operations the monitor allows. 

By default, a KVM guest will receive an IP address within the 10.0.2.0/24 range, and have outgoing access (including SSH access) both to its host, and to the wider network beyond. However, also by default, it won't be able to host incoming requests for network clients. If you need to open up incoming network connectivity, you'll probably want to create a network bridge on your host that's similar to the one we used for Xen in the previous module. As before, you will install bridge-utils and, assuming you want your host to receive its IP from a network DHCP server, edit the /etc/network/interfaces file to look something like this: 

That's how it will work on Debian/Ubuntu machines. Other distributions might require editing different files in the /etc/sysconfig/network-scripts/ directory or using GUI network setup tools. Once you've configured your network, you will stop and restart your network services or just reboot. 
To add an existing guest to a new bridge, you can edit the guest's .XML file so that, instead of a value of "default" on your "source network" line, it will match the name of your host's network bridge: 

Now that the bridge is up and running, you tell KVM to associate your client to it by adding this -net argument using qemu-system-x86_64: 
And if you're working with virt-install, you will add the argument this way: 

KVM guests can run quite happily using their own internal resources. Nevertheless, you might sometimes need to provide a guest with access to shared resources residing on the host, or to remote network resources. You can define such resources and effectively mount them within a virtual machine by incorporating the contents of a .XML file into your client's definition. This command tells the virsh tool "pool-define" to load the configuration found in a file called shared_files_disk.xml in your home directory: 
	sudo virsh pool-define ~/shared_files_disk.xml 
Here's how such a file might be structured. Libvirt organizes storage resources as pools, dividing them into volumes that can be represented to a guest as block devices. So, for example, you could include a <pool> entry in your .XML configuration of the "netfs" type, that exposes a directory path on a remote machine - /home/datauser/current-files on a domain called nfs.example.com, for example - to the /var/current-files directory mount point on the guest: 

The mounted data, once started from the host using pool-start, will be accessible to the libvirt API as a volume.
Besides the netfs (Network File System), other valid KVM/libvirt pool types include Directory (DIR), iSCSI server (iSCSI), and Logical volume storage pool (Logical). 

Let's review. To make sure that your server supports the vt/svm instruction set, run kvm-ok. You'll also need to load the kernel module kvm and either kvm-intel or kvm-amd. 
Virsh works for KVM hypervisors just as for other libvirt environments. The virt-manager package includes virt-install, which can be used to provision domains. You can, for instance, specify a console using the --extra-args argument. You can start and stop guests using virsh.
The KVM Management Tool package includes vmbuilder with its own special syntax. You can also load your configuration details into a .cfg file and run vmbuilder against that.
The qemu-kvm toolset uses qemu-img to create a disk image and then kvm to generate a .XML file in the directory you specified. You use qemu-system-x86_64 -monitor stdio to launch a shell from which you can monitor your KVM resources. Network connectivity for your guests is defined  through the -net bridge setting, which points guest domains to your host's network bridge.
Virsh pool define, when run against a .xml file for a guest, will mount a remote host file system locally. virsh pool-start must be run on the host machine.

