Hi there. I'm David Clinton and I'm here to talk about Linux server virtualization.
Through the course of an average day, billions of virtual machine instances of one kind or another are being launched as part of some kind of public or private cloud or computing cluster. But it's not only the big players like Google or Amazon Web Services who are investing ever-greater resources into hypervisor virtualization: as cloud-based applications and Internet of Things integration increasingly dominate compute models, widespread access to high-speed network connectivity and cheap data storage are driving virtualization into smaller server rooms and inspiring more local resource planning.
If you want a piece of all this action, you'll need to start with a solid understanding of the inner workings of server virtualization and hyper-scale distributed computing. And since, one way or another, Linux is doing most of the heavy lifting in this new virtual world, Linuxland is where you'll want to be.
In this course, we will explore the key concepts and technologies involved in Linux server virtualization, including hypervisor platforms like Xen and KVM, and container tools like LXC and Docker. Since I don't happen to have a server room filled with racks of bare metal servers at my disposal, I'll actually use virtualization tools myself to demonstrate the installation and configuration of some basic hypervisor-based projects. While the layers of un-reality may get a little bit Alice-in-Wonderland-like at times, I'll try my best to keep everything nice and clear.
As virtualization has justifiably earned a reputation for being really complicated, I'm going to spend some time just describing the structure and goals of each of the technologies we'll use. Sometimes, as you will see, the complexity isn't so much in getting things up and running, but in visualizing the exact role each part is supposed to play.
To illustrate all this, I'll pretend that I'm part of the team responsible for IT services at a small college, getting ready to finally move its Human Resource, email, and learning management systems out of the twentieth century. With a limited budget, they'd definitely rather not have to purchase standalone servers they might only need periodically to meet predictable cycles of use. The idea of reallocating the resources used to meet their start-of-year enrollment demands to the Moodle content platform during the three-times-a-year pre-exam rush sounds great. And so does the ability to spin up new servers - both test and production - almost instantly. The cost savings involved in running a full stack of open source software won't cause anyone to lose sleep either.
Like most technology training, just watching me having fun here won't help you all that much. If *you* want to master at least some of these skills for yourself, you'll have to try them out on your own. Even better, rather than just reproducing what I've done, plan and create your own variations building on what you see me doing. The more failures and mistakes you encounter, the closer you'll be to mastering the topic.
Finally, while this course is a generic introduction to virtualization, by and large, it does happen to follow around half of the the topics required by the Linux Professional Institute's LPIC-3 304 certification exam expectations. I expect to produce a second course covering the rest in the near future. So if you're in the market for some extra professional credibility, you might want to consider formalizing your knowledge with an industry-recognized cert.
I've never seen this explicitly, but I suspect that "hypervisor" is a play on the word "supervisor" and, since "hyper" means "super" a hypervisor would be a super-super-visor. 
Technically, a hypervisor is a software layer that allocates the hardware resources of a single bare-metal server (a host) among multiple virtualized operating systems (guests). This architecture allows you to achieve much higher server density (which means you can use a single hardware server to provide applications for all kinds of uses and clients, rather than just having to focus on one role) and much more efficient use of resources like CPUs, RAM, and storage drives. But, since your virtual guests are all effectively playing in the same sandbox together, there can also be a greater risk of data "leakage" between guests, or unauthorized access to the host kernel. 
Therefore, when building a hypervisor, you should remember that the consequences of a vulnerability effecting the host are far greater than it would be against a stand-alone server, because the host operating system has full access to all of its guests. It is for this reason that major design goals for hypervisors include end-to-end fault isolation (to prevent an application failure in one virtual machine from affecting others), performance isolation (to ensure that each virtual machine has reliable access to the resources it needs), and consistency.
You should also create some kind of practical control mechanism to prevent "server sprawl" - where the ease of spinning up new VMs makes it likely that more and more forgotten and unmanaged guests will end up cluttering your host machines. Each forgotten VM holds on to its share of resources and exposes more potential security holes. As I myself have seen more than once, the larger your operation - and the more admins and dev-ops types who work on it - the greater the risk is that processes and VMs will be left running.
There have traditionally been two classes of hypervisor: Type-1 and Type-2. Bare-metal hypervisors (Type-1) maintain full control over the host hardware, running each guest OS as a system process. Xen and VMWare's ESXi are prominent modern examples of Type-1. Although ESXi - also known by the name of its parent product: vSphere - is a leading player in the hypervisor market that runs on top of a modified Linux kernel, it won't be covered in this course. In recent years, use of the term "hypervisor" has spread to include all host virtualization technologies, but some time back, it would have been used to describe only Type-1 systems. The more general term covering all types would originally have been "Virtual Machine Monitors". If people use the term Virtual Machine Monitors at all these days, I suspect they mean "hypervisor" in all its forms. 
Hosted hypervisors (Type-2) are themselves simply processes running on top of a normal operating system stack. Type-2 hypervisors (which include VirtualBox, QEMU and, perhaps KVM) abstract host system resources for guest operating systems, providing the illusion of a private hardware environment. 
The important practical difference between these two approaches is how directly the hypervisor can control the host hardware, as working through layers of emulation to access physical resources can add latency to a system. Therefore, in theory at least, the more "aware" a hypervisor is of its physical environment, the more efficiently it will be able to get things done.
Paravirtual (or PV) guests are at least partially aware of their virtual environment, including the fact that they're sharing hardware resources with other virtual machines. This awareness means that there's no need for PV hosts to emulate storage and network hardware and means that more efficient I/O drivers can be employed. Historically, this has allowed Paravirtual hypervisors to achieve better performance for those operations requiring connectivity to hardware components. 
Hardware Virtual Machines (HVM), on the other hand, are fully virtualized. Or, in other words, if you would tell them that they weren't actually all alone on their host servers, they would stare at you with blank incomprehension (assuming they had eyes and facial expressions, of course). The main advantage of this arrangement is that HVM hypervisors don't need to interface with their environment any differently than a standalone OS might, so they can run with standard, unmodified software stacks. The down side is, of course, that translating hardware signals through an emulation layer takes extra time and cycles. 
However, modern hardware architectural designs have introduced a new library of virtualization-friendly CPU instruction sets. With this greater hardware support, HVM virtualization has pretty much overcome the emulation bottleneck and can take full advantage of hardware extensions and unmodified software kernel operations. 
Therefore, the HVM model is now recommended for most use cases, and can deliver greater portability and compatibility. In fact, Amazon's EC2 is making it harder and harder to even find PV-based machine instances.

As we've seen, a hypervisor-based VM (or guest) is a complete virtualized operating system that thinks it's running on its own computer. A container, on the other hand is an *application* that thinks it's an operating system. In container technologies (like LXC and Docker), containers are nothing more than a software abstraction relying on the host kernel and hardware resources for everything they do. Of course, this means that containers can only run using the host's kernel - so virtualizing Windows - or even older or more recent Linux releases - on, say, an Ubuntu 14.04 host - is impossible.
What containers *can* do is run versions of multiple Linux distributions that rely on the host kernel. So, for instance, you can use pre-built templates to run compatible versions of Fedora, Ubuntu, or OpenSUSE on a Debian LXC host.
You can think of containers - and especially Docker - as heavily scriptable packaging standards for individual applications. If, for instance, you want a web server as a front end for a MySQL database that's running on a separate data server, you can precisely define all the dependencies and network details in advance (using a dockerfile script in Docker's case) and the container will boot and run without the need for any direct, hands-on administration. 
Containers do have their design limits. Because they don't necessarily scale as easily as hypervisor VMs, they can be better suited for delivering micro services than full-sized enterprise application environments. And, as relatively young technologies, there might still be some security surprises waiting for us as active production deployments increase. Be prepared.

Besides build-your-own virtualization solutions, you can also out-source data services to public cloud providers. You should be familiar with those options both because one of them might be a good fit for your own project, and because - who knows - your virtualization skills might one day land you a great job at a cloud provider like AWS or Google.
In general, cloud computing services allow you to avoid having to estimate potential peak demand levels and then purchase and provision expensive hardware resources to match them. Instead, administrators can consume only the services they need, when they need them, with no up-front capital expense costs. 
Server virtualization technologies have made such services practical and, at the same time, allowed for a business model based on automation and micro-billing that makes it financially attractive. The basic idea is that virtualized units of compute, data, networking, and security tools are purchased as needed and billed per use. Since everything is virtual, scaling up or down is pretty much instant, and can be heavily automated. Cloud providers like Amazon Web Services offer services packaged to look and work in ways familiar to server room admins, while providing system-wide integration and geographically distributed duplication that previously couldn't have been imagined. 
New cloud services appear regularly, often reflecting really creative thinking. But three models in particular currently dominate the cloud world. 
Infrastructure as a Service makes virtualized hardware available which you administrate just as you would a physical server. IaaS providers like AWS (Elastic Cloud Compute - EC2), Microsoft Azure (Virtual Machines), and the Google Cloud Platform (Compute Engine), offer images of the operating system of your choice to go with their virtualized compute instances. When placed within an infrastructure ecosystem including network and firewall services, you are free to do just about anything you want - at any scale you need. 
While all major IaaS providers offer Windows machine images, as you might expect, the vast majority of the millions of server instances out there in the wild are really running one distribution or another of Linux, with Ubuntu being the most popular by far. 
A "platform" in the cloud world, is a web service that manages your project's underlying infrastructure for you. Platform as a Service lets you create and manage your own applications without having to worry about the compute and network ingredients that will make them run. You will usually need to select a particular software environment like Java, Python or PHP, and then upload your code – which is how Amazon's Elastic Beanstalk works. Other PaaS services - like Salesforce.com's Lightening App Builder - offer a set of abstracted tools from which you can assemble user applications. Either way, you only have to worry about the public function of your application. 
Many of the most widely-used cloud-based tools fall into the category of Software as a Service, because they are provided as complete services. Google's Gmail and Drive and the WordPress.com web publishing platform are excellent examples of software that's ready and available for immediate, on-demand use by consumers. 

At least some virtualization features require hardware support - especially from the host's CPU. Therefore, before getting started, you should make sure that whatever you're using as a server has what it needs to get your job done. The /proc/cpuinfo file has most of the relevant information, in particular, in the "flags" section of each processor. Since there will be so many flags however, you'll need to know what to look for. 

The "svm" flag in this output tells me that I've got AMD-V support. ("vmx" would indicate the Intel equivalent.) "lm" means that I've got a 64-bit processor, and "aes" refers to Accelerated AES. 
Part of the beauty of virtualization is how easy it can be to duplicate whole server environments in multiple locations and for multiple purposes. Sometimes it's no more complicated than creating an archive of a virtual file system on one host (usually found within the /var/lib hierarchy), unpacking it within the same path on a different host, checking the basic network settings, and firing it up. Other platforms, like Xen, offer a single command line operation to move guests between hosts.
However, migrating environments from physical servers to a hypervisor of one sort or another can sometimes be a bit more tricky. Even if a simple lift-and-shift of the relevant data from a locally hosted LAMP web server (typically consisting of the /etc and /var directory trees) to, say, a LAMP server running on the AWS cloud should run right out of the box, you might still need to make considerable design adjustments to take advantage of all the real AWS functionality (including managed databases, multi availability zone replication, and security groups). 

Let's review what we've seen so far. A type-1 hypervisor, like Xen and VMware's ESXi, runs directly on host hardware, running virtual machines as system processes without the need for a hardware emulation layer, while type-2 hypervisors are themselves processes on a standard software stack and, as such, can run conflict-free across more platforms. Containers share the host kernel, which makes building and launching them much faster.
Infrastructure as a Service is a cloud computing model that provides highly scalable compute services through virtualized machine instances. Platform as a Service tools take care of underlying infrastructure for you, allowing you greater simplicity in designing and provisioning your applications. The Software as a Service model offers complete cloud-based consumer services like business communications and platform building.
Before building a virtualization host of one sort or another, you'll need to make sure your hardware can manage it. The /proc/cpuinfo file is a good place to start.

